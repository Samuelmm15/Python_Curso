{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb_Kfh-plQeA"
      },
      "source": [
        "# Inteligencia artificial\n",
        "\n",
        "La inteligencia artificial (IA) hace posible que las máquina aprendan de la experiencia, se ajusten a nuevas aportaciones y realicen tareas como hacen los humanos. La mayoría de los ejemplos de inteligencia artificial de los que usted escucha hoy día – desde computadoras que juegan ajedrez hasta automóviles que se conducen por sí solos – se sustentan mayormente en aprendizaje a fondo (deep learning) y procesamiento del lenguaje natural. Mediante el uso de estas tecnologías, las computadoras pueden ser entrenadas para realizar tareas específicas procesando grandes cantidades de datos y reconociendo patrones en los datos.\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1e8cKqzwywt1OKqW1rfLLT6chdSuB1Hx_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-YXplrarwuQ"
      },
      "source": [
        "# Web Scraper\n",
        "\n",
        "Cuando los datos no los tienes en una base de datos sino están colgados en una web y ni siquiera hay API no queda otra que extraerlos directamente de la web. Eso es el web scraping, básicamente vamos a una URL y extraemos de su código HTML lo que queramos. Esto da solución, las API y el web scraping, al problema de los datos, que en su mayoría son **no estructurados**, es decir, no nos vienen dados con un formato csv, txt o excel de una web, sino vienen en bruto y tenemos que hacer todo el proceso de manejarlos nosotros. Esto viene dado porque mucho de los datos actuales proceden de redes sociales (ya sean las más convencionales, Instagram, Twitter, Tik tok, o de otras como Tripadvisor, Booking, Airbnb, etc.)\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1L7d0ehVPmgjzs--od-5r8bDw6HGZHwsb)\n",
        "\n",
        "Se puede decir que también es un Bot, porque simula una acción humana. Es decir, estamos programando a una máquina para hacer algo que podríamos hacer manualmente pero muy rápido.\n",
        "\n",
        "![hero-recaptcha-demo.gif](https://drive.google.com/uc?export=view&id=16zrPHCLOVAoQA74B_U-tcCo4kJK3s5H1)\n",
        "\n",
        "Vamos a usar técnicas de web scraping para poder obtener los comentarios de los clientes de Tripadvisor, con el fin posterior de clasificar si su comentario es bueno o negativo a posteriori.\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1mdVm72iV5rG1ZseWltnQ-ql6NwJyBP3E)\n",
        "\n",
        "\n",
        "Como Tripadvisor no nos deja ninguna API para poder descargarlo tenemos que optar a este tipo de técnicas. En concreto hemos cogido dos restaurantes al azar de la isla y los hemos incluido para su scrapeo.\n",
        "\n",
        "Importante en este caso que nos enfrentamos a un código más complejo, formado por una clase que se encargará de las funcionalidades y por otro lado vemos el\n",
        "```if __name__ == '__main__':``` que utilizado cuando quieres ejecutar un programa Python. Para los que saben otros lenguajes de programación es la función main. Cuando se ejecute desde terminal ```programa.py``` va a buscar a esta función. En concreto para ejecutar un bloque en Jupyter no haría falta, pero viene bien que vean el ejemplo de como se haría de verdad en un script *.py* \n",
        "\n",
        "Para el scraping se va a usar **selenium** que descarga todo el HTML de la página que le indiquemos, y **BeautifulSoup** que coge ese HTML y lo transforma a un formato donde será capaz de buscar las etiquetas que le indiquemos con el nombre de algún campo si hiciera falta.\n",
        "\n",
        "HTML es un lenguaje de etiquetas, donde se ponen elementos según su utilidad. Ejemplo \\<a>\\</a> sirve para poner un enlace \\<table>\\</table> una tabla o \\<p>\\</p> un párrafo.\n",
        "\n",
        "![etiqueta_html.png](https://drive.google.com/uc?export=view&id=1wpW1pDIyXTbLIl6MzX3haPTnBmD-fvTx)\n",
        "\n",
        "Para ver el código html de cualquier página web, apretar boton derecho la web y apretar inspeccionar. Si no nos sale por defecto, apretar la pestaña de \"elementos\"\n",
        "\n",
        "![inspeccionar.png](https://drive.google.com/uc?export=view&id=1dQX42MAV8RXmIxbOdN33kBP1QooTDn9G)\n",
        "\n",
        "\n",
        "El programa va a acceder a las páginas del restaurante que le indiquemos. Para eso le indicamos código de zona y código de restaurante. Con eso formamos la URL, porque en concreto en tripadvisor se repite que las webs van a estar formadas por\"https://www.tripadvisor.co.uk/Restaurant_Review-{código_zona}-{código_restaurante}-Reviews-or{número_página_comentario}\".\n",
        "\n",
        "Cada página tiene diez comentarios visualizados. Por ejemplo, en la que tenega \"número_página_comentario\" igual a 0 se mostrarán los comentarios de 0 a 9. Cuando sea 10, de 10 a 20. Y así sucesivamente.\n",
        "\n",
        "Desde la página principal del restaurante no se visualiza todo el comentario completo, por eso accedemos a otra página luego, la propia del comentario. Ahí sí podremos buscar el campo donde está el texto implícito del comentario y almacenarlos en nuestro objeto pandas que habremos creado previamente.\n",
        "\n",
        "Las clases tienen variables propias llamadas self. Son variables que podrémos llamar en todo el ámbito de la clase y que se inicializan en el *__init__()*. Cuando creamos una clase estamos llamando a *__init__* implícitamente. Por eso le pasamos argumentos, porque en *__init__()* los estamols pidiendo. Luego, tenemos que acceder a sus métodos manualmente, por ejemplo en este caso tenemos **scraperTrip.get_comentarios()**. Siempre que hemos visto en librerías colocar, por ejemplo con pandas, *pd.read_csv()*, entrábamos a un método de una clase.\n",
        "\n",
        "Al final obtendremos un Dataframe de Pandas con los datos de los comentarios de los hoteles que hayamos indicado. Sus columnas serán:\n",
        " - cod_zona. Código de zona.\n",
        " - cod_rest. Código de restaurante.\n",
        " - cods_revs. Código de comentario.\n",
        " - revs. Comentario.\n",
        "\n",
        "La forma en la que comento la funcionalidad de cada función es el estandar no oficial de Python (acuérdense que en Python no hay normas escritas sino recomendaciones). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlu4FmH9Lg7p",
        "outputId": "fc029433-bdde-49ae-c4b2-a6ed7991f3fd"
      },
      "outputs": [],
      "source": [
        "# !pip install selenium\n",
        "# !apt-get update # to update ubuntu to correctly run apt install\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nnfyWdRsDUDJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restaurante url: https://www.tripadvisor.co.uk/Restaurant_Review-g662606-d14091847-Reviews-or0\n",
            "Página 1 de 6 comentarios\n",
            "Comentarios 10\n",
            "Página 2 de 6 comentarios\n",
            "Comentarios 20\n",
            "Página 3 de 6 comentarios\n",
            "Comentarios 30\n",
            "Página 4 de 6 comentarios\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-87245f7f316a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m                           ]\n\u001b[1;32m    188\u001b[0m      \u001b[0mscraperTrip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscraperTripadvisorRestaurantes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_restaurantes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m      \u001b[0mdf_comentarios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscraperTrip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_comentarios\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_comentarios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-87245f7f316a>\u001b[0m in \u001b[0;36mget_comentarios\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \"\"\"\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Obtenemos lista de ids de comentarios que guardamos en self.URL_rev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ids_comentarios\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Añadir comentario en sí desde su dirección concreta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-87245f7f316a>\u001b[0m in \u001b[0;36mget_ids_comentarios\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0;31m# Inicializamoz el soup para seleccionar etiquetas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_html_web\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;31m# Vamos a obtener sólo el id del comentario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-87245f7f316a>\u001b[0m in \u001b[0;36mget_html_web\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mchrome_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--no-sandbox'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mchrome_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--disable-dev-shm-usage'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chromedriver'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             log_path=service_log_path)\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mWebDriverException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not connect to the Service %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# import urllib.request\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "# Todos los restaurantes y comentarios de los propios tienen el mismo formato de URL\n",
        "\n",
        "class scraperTripadvisorRestaurantes():\n",
        "\n",
        "    def __init__(self, lista_restaurantes):\n",
        "        \"\"\"\n",
        "            :params lista_restaurantes:   lista con todos los códigos de \n",
        "                                          restaurantes y sus zonas que vamos\n",
        "                                           a visitar.\n",
        "\n",
        "            Inicializo la clase. Aprovechamos que en este caso se repite siempre\n",
        "            la estructura de los restaurantes para poner la URL con llaves que \n",
        "            luego sustituiremos con el método \"format()\"\n",
        "\n",
        "            - El primer corchete corresponde al código de la zona\n",
        "            - El segundo corchete al código del restaurante\n",
        "            - El tercer corchete, en el caso de los restaurantes, \n",
        "              se refiere a la página de los comentarios.\n",
        "            - El tercer corchete, en el caso de la URL de las reviews,\n",
        "              al del comentario.\n",
        "        \"\"\"\n",
        "        # Definimos las bases que vamos a usar para URL de restaurantes y de\n",
        "        # revisiones\n",
        "        self.URL_base_rest = \"https://www.tripadvisor.co.uk/Restaurant_Review-{}-{}-Reviews-or{}\"\n",
        "        self.URL_base_rev = \"https://www.tripadvisor.co.uk/ShowUserReviews-{}-{}-{}\"\n",
        "        self.lista_restaurantes = lista_restaurantes\n",
        "        # Inicializamos lista con los restaurantes\n",
        "        self.URL_rest = []\n",
        "        # Añadir los restaurantes\n",
        "        for restaurante in lista_restaurantes:\n",
        "            # Al inicio todas las páginas empiezan por la 0\n",
        "            self.URL_rest.append(self.URL_base_rest.format(restaurante['cod_zona'], \n",
        "                                                           restaurante['cod_rest'],\n",
        "                                                           0))\n",
        "        # Inicializamos el DataFrame que vamos a devolver.\n",
        "        # En este caso empezará vacío, solo indicaremos las columnas\n",
        "        self.revs = pd.DataFrame(columns=['cod_zona','cod_rest',\n",
        "                                          'cods_revs','revs'])\n",
        "\n",
        "    def get_html_web(self, url):\n",
        "      \"\"\"\n",
        "        Devolver el código html.\n",
        "        Se obtiene código con la librería selenium y se transforma a un objeto\n",
        "        BeatifulSoup para poder tratarlo luego.\n",
        "        :arg url: Url que se desea obtener\n",
        "        :return soup: código html parseado por la librería BeatifulSoup.\n",
        "      \"\"\"      \n",
        "      chrome_options = webdriver.ChromeOptions()\n",
        "      chrome_options.add_argument('--headless')\n",
        "      chrome_options.add_argument('--no-sandbox')\n",
        "      chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "      driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "      driver.get(url)\n",
        "\n",
        "      soup = ''\n",
        "      while soup == '':\n",
        "        time.sleep(0.1)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "      driver.quit()\n",
        "\n",
        "      return soup\n",
        "\n",
        "    def get_comentarios(self):\n",
        "        \"\"\"\n",
        "          Devolver comentarios\n",
        "          :return comentarios: diccionario con todos los comentarios \n",
        "                               por restaurantes\n",
        "        \"\"\"\n",
        "        # Obtenemos lista de ids de comentarios que guardamos en self.URL_rev\n",
        "        self.get_ids_comentarios()\n",
        "\n",
        "        print('Añadir comentario en sí desde su dirección concreta')\n",
        "        # ===================================================\n",
        "        # ===================    3    =======================\n",
        "        # ====== Obtenemos los comenarios en sí mismos ======\n",
        "        # ===================================================\n",
        "        for i in range(0,len(self.revs)):\n",
        "\n",
        "\n",
        "            # Obtenemos todo el html de la página\n",
        "            url = self.URL_base_rev.format(self.revs['cod_zona'].iloc[i],\n",
        "                                           self.revs['cod_rest'].iloc[i],\n",
        "                                           self.revs['cods_revs'].iloc[i])\n",
        "            print('pagina del comentario',url)\n",
        "            # Inicializamos el soup para seleccionar etiquetas\n",
        "            soup = self.get_html_web(url)\n",
        "            comentario_campo = soup.findAll(\"p\", {\"class\": \"partial_entry\"})\n",
        "            comentario_texto = comentario_campo[0].text\n",
        "\n",
        "            self.revs['revs'].iloc[i] = comentario_texto\n",
        "            \n",
        "        return self.revs\n",
        "\n",
        "    def get_ids_comentarios(self):\n",
        "        \"\"\"\n",
        "            Devuelve los id de los comentarios que aparecen en cada página de\n",
        "            los restaurantes y con los que se puede saber la página URL\n",
        "            para acceder\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        for i in range(0, len(self.URL_rest)):\n",
        "            print('Restaurante url:', self.URL_rest[i])\n",
        "            # Obtenemos todo el html de la página e inicializamoz \n",
        "            # el soup para seleccionar etiquetas\n",
        "            soup = self.get_html_web(self.URL_rest[i])\n",
        "            \n",
        "\n",
        "            # ===================================================\n",
        "            # ===================    1    =======================\n",
        "            # = Obtenemos el número de páginas con comentarios ==\n",
        "            # ===================================================\n",
        "\n",
        "            # Si no hay páginas entonces lo indicamos\n",
        "            try:\n",
        "                n_comentarios_elemento = soup.findAll(\"div\", {\"class\": \"ui_header h4 counts\"})\n",
        "\n",
        "                # Coger el texto de ese elemento el primero, y del \"X results\" \n",
        "                # que obtenga solo el \"X\"\n",
        "                n_comentarios= n_comentarios_elemento[0].text.split()[0]\n",
        "\n",
        "                # Redondeamos a la decena más cercana, pues cada bloque de\n",
        "                # comentarios viene de diez en diez. Por ello también\n",
        "                # dividimos, pues queremos saber el número de páginas\n",
        "                # de cometarios\n",
        "                n_pag_comentarios = int(round(int(n_comentarios),-1)/10)\n",
        "\n",
        "            except:\n",
        "                n_pag_comentarios = 0\n",
        "\n",
        "            # ===================================================\n",
        "            # ===================    2    =======================\n",
        "            # ===  Obtenemos las páginas de cada comentario =====\n",
        "            # ===================================================\n",
        "            for n_pag_comentario in range(0, n_pag_comentarios):\n",
        "                  \n",
        "                # Obtenemos todos los campos con comentarios\n",
        "                comentarios = soup.findAll(\"div\", {\"class\": \"review-container\"})\n",
        "                print('Página',n_pag_comentario+1,'de', n_pag_comentarios,'comentarios')\n",
        "\n",
        "                # Obtenemos todos los campos con comentarios\n",
        "                for comentario in comentarios:\n",
        "                    # Obtenemos todo el html de la página\n",
        "                    url = self.URL_base_rest.format(self.lista_restaurantes[i]['cod_zona'],\n",
        "                                                    self.lista_restaurantes[i]['cod_rest'],\n",
        "                                                    n_pag_comentario)\n",
        "\n",
        "                    # Inicializamoz el soup para seleccionar etiquetas\n",
        "                    soup = self.get_html_web(url)\n",
        "                    \n",
        "                    # Vamos a obtener sólo el id del comentario\n",
        "                    # porque aquí no lo podemos descargar enteros desde aquí.\n",
        "                    # Este es el campo donde está el enlace y el id\n",
        "                    campo_enlace = comentario.findChildren(\"a\" ,{\"class\":\"title\"})\n",
        "                    # Obtengo y guardo id comentarios para luego buscarlo\n",
        "                    id = campo_enlace[0].get('id')\n",
        "                    # El id comentario tiene forma por ejemplo \"rn778259474\", pero\n",
        "                    # nosotros solo queremos \"r778259474\".\n",
        "                    # Todos empiezan \"rn\" más el número, así que eliminamos la n\n",
        "                    id = id.replace('n', '')\n",
        "                    self.revs=self.revs.append({'cod_zona':self.lista_restaurantes[i]['cod_zona'], \n",
        "                                                'cod_rest':self.lista_restaurantes[i]['cod_rest'], \n",
        "                                                'cods_revs':id, \n",
        "                                                'revs':''} , ignore_index=True)\n",
        "\n",
        "                print('Comentarios',len(self.revs))\n",
        "                \n",
        "\n",
        "# # Ejecutar la aplicación principal\n",
        "if __name__ == '__main__':  # Función principal del programa\n",
        "     # Vamos a sacar dos restaurantes en concreto.\n",
        "     # Se podría también hacer un scraper de todos los restaurantes \n",
        "     lista_restaurantes = [\n",
        "                           {'cod_zona':'g662606', 'cod_rest':'d14091847'},\n",
        "                           {'cod_zona':'g796999', 'cod_rest':'d15662340'},\n",
        "                           {'cod_zona':'g659661', 'cod_rest':'d15677484'},\n",
        "                           {'cod_zona':'g659661', 'cod_rest':'d13169911'}\n",
        "                          ]\n",
        "     scraperTrip = scraperTripadvisorRestaurantes(lista_restaurantes)\n",
        "     df_comentarios = scraperTrip.get_comentarios()\n",
        "     print(df_comentarios)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "J3QM393_owoZ",
        "outputId": "cc1438c9-44fa-453b-cc70-b35fb3910e59"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_comentarios' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e66cc332f5b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Si imprimimos el DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_comentarios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_comentarios' is not defined"
          ]
        }
      ],
      "source": [
        "# Si imprimimos el DataFrame\n",
        "df_comentarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc7PptW4xVFq"
      },
      "source": [
        "Podemos si queremos almacenar un csv de la siguiente manera. \n",
        "\n",
        "Si no estuvieramos en Colab bastaría con poner df_comentarios.to_csv('nombre_archivo.csv'). En el resto de líneas estamos pidiendo permisos para poder almacenarlo en nuestro Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_iNTOy2Oyei",
        "outputId": "bbac99d9-f629-4a99-f825-edfe136bb7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Guardar CSV en el Colab\n",
        "# Tienes que pinchar el enlace que te sale, aceptar con tu cuenta y después copiar\n",
        "# la salida que te da y pegarla en el recuadro.\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "df_comentarios.to_csv('comentarios_tripadvisor130.csv')\n",
        "!cp comentarios_tripadvisor130.csv \"drive/My Drive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ6YqhATa3hm"
      },
      "source": [
        "Por si va muy lento la extracción dejo el csv resultante subido. Lo dejo comentado por si les da tentación de ejecutarlo sin leer y no les vaya a fastidiar.\n",
        "\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1Ho7ck3Wbt1TpwFonRtLqZRoJRijB8dwI)\n",
        "\n",
        "PD: Acuérdense de que para leer excel el comando con Pandas es\n",
        "pd.read_excel() en vez de read_csv(), que en las clases solo hemos visto el caso de csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7Hr82-qagIo"
      },
      "outputs": [],
      "source": [
        "# Si quieren leer directamente el csv, \n",
        "df_comentarios = pd.read_csv('https://raw.githubusercontent.com/SerDiaz/Introduccion_programacion_en_python/main/dia5/comentarios_tripadvisor130.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtH3eQYVxvhR"
      },
      "source": [
        "### Ejercicio 1\n",
        "Prueba a buscar dos restaurantes más y añadirlos al scrapeo y guardar el nuevo csv generado en tu ordenador (con que me pongan el código que tendrían que usar me vale, que ya sabemos que el ejecutar tarda. Aunque si lo quieren probar mientras explico mejor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMUCOg8vPugr"
      },
      "outputs": [],
      "source": [
        "# Resolver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BzGnJSiKTVR"
      },
      "source": [
        "### Ejercicio 2\n",
        "En este ejercicio vamos describir como realizar un scrapeo. Como si fuéramos detectives.\n",
        "\n",
        "![conan.png](https://drive.google.com/uc?export=view&id=1P2sKXmNI1cqh-ycq1jM7bzOkpVu5gHh6)\n",
        "\n",
        "Para ejemplificar lo que me refiero, voy a describir como quedaría explicado el proceso que hemos llevado a cabo para obtener los comentarios de los restaurantes de tripadvisor con lenguaje natural: \n",
        "\n",
        "**1 )** Accedo a la web de un restaurante cualquiera, por ejemplo https://www.tripadvisor.co.uk/Restaurant_Review-g662606-d14091847-Reviews-Restaurante_Donaire-Costa_Adeje_Adeje_Tenerife_Canary_Islands.html\n",
        "\n",
        "**2)** Los comentarios se encuentran al final de la página. En concreto vienen divido por páginas por las que puedo navegar interaccionando con los botones dispuestos. En cada página hay un máximo de 10 comentarios.\n",
        "\n",
        "**3)** Al apretar cada página de comentarios cambia la URL. Por tanto no tengo que simular los clicks con Selenium, sólo acceder a la URL. \n",
        "\n",
        "   Ejemplo, la URL para los comentarios de 10 al 20 es https://www.tripadvisor.es/Restaurant_Review-g662606-d14091847-Reviews-or10-Restaurante_Donaire-Costa_Adeje_Adeje_Tenerife_Canary_Islands.html, para los siguientes 10 https://www.tripadvisor.es/Restaurant_Review-g662606-d14091847-Reviews-or20-Restaurante_Donaire-Costa_Adeje_Adeje_Tenerife_Canary_Islands.html\n",
        "\n",
        "**4)** Comparando la URL con la de otros restaurantes, puedo deducir que lo único que cambia de las URL, además del nombre al final, es el \"g662606\" y el \"d14091847\", además del \"or20\" que nos indica la página de comentarios actual. \n",
        "\n",
        "   Testeo que solo escribiendo hasta el \"or20\" ya me reedirige al restaurante, evitando tener que escribir el \"Restaurante_Donaire-Costa_Adeje_Adeje_Tenerife_Canary_Islands.html\n",
        "\", por lo que el código puedo hacerlo más elegante. \n",
        "\n",
        "   También comparando con otros restaurante acabo deduciendo que el \"g662606\" es el código de la zona, y que el \"d14091847\" es el código del restaurante.\n",
        "\n",
        "**5)** Clickeando cada comentario te lleva a una página con una url única como esta: https://www.tripadvisor.co.uk/ShowUserReviews-g662606-d14091847-r812486250-Restaurante_Donaire-Costa_Adeje_Adeje_Tenerife_Canary_Islands.html\n",
        "\n",
        "Esta url comparte el código de zona y de restaurante anterior, pero añade un nuevo campo, el \"r812486250\", que después de comparar con otros comentarios, entiendo que es un identificador único.\n",
        "\n",
        "Por tanto, para guardar todos los comentarios, sólo necesito guardar este identificador.\n",
        "\n",
        "**6)** La cadena por tanto será la siguiente: \n",
        "\n",
        "    a) Abrir la página del restaurante. \n",
        "\n",
        "    b) Obtener la página de cada comentario.\n",
        "\n",
        "    c) Cambiar de página de comentarios abriendo la siguiente página.\n",
        "    \n",
        "    d) Repetir el proceso hasta tener almacenado todas las páginas de comentarios para cada restaurante buscado.\n",
        "\n",
        "    e) Abrir cada página de comentarios y extraer el comentario en sí, con los identificadores correspondiente \n",
        "\n",
        "**7)** Ahora especifico las etiquetas en sí mismo a extraer. Buscando en el código html de la página el identificador \"812486250\", me doy cuenta que hay varios bloques donde puedo conseguir el identificador. Me voy a decantar por buscar todas las etiquetas **a** donde el nombre del atributo **class** sea **title**. En la misma etiqueta tenemos otro atributo **id** donde viene el identificador, aunque en este caso escrito como \"rn812486250\" en lugar de \"r812486250\". Así que tenemos que borrar la \"n\". Con eso tendremos los comentarios.\n",
        "\n",
        "**8)** Para cada página, puedo hacerlo de varias formas. En mi caso, para hacer un bucle, necesito saber la última página de comentarios. Sólo se puede interactuar con 6 páginas. Si hay más páginas es imposible saber el fin a no ser que interactúe.\n",
        "\n",
        "En un lado de la página aparecen el número de reviews, pero veo que no cuadra con el número de comentarios visibles (entiendo que están mostrando sólo los últimos comentarios desde cierta fecha). En código HTML sí encuentro un lugar donde dice exactamente el número de comentarios visibles \"58 results\". Sabía que eran 58 porque en la página 6, la última, sólo había 8 comentarios visibles. el \"58 results\" está en la etiqueta **div** cuyo atributo **class** es **ui_header h4 counts**. Una vez obtenido el número \"58\", solo tengo que redondear la decena a \"60\", y dividir por 10 para saber el número de páginas totales y hacer el bucle.\n",
        "\n",
        "**9)** Para obtener los comentarios en sí hago el inspeccionar, en la página de cada comentario, el texto del comentario. Me fijo que el comentario está en la etiqueta **p** cuyo atributo **class** tiene por nombre **partial_entry**. \n",
        "\n",
        "\n",
        "\n",
        "### **Ahora, el ejercicio a realizar ustedes consiste en anotar los pasos necesarios para lograr realizar el scraping de todos los restaurantes de la isla de Tenerife**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2Yy2aYNkgk"
      },
      "source": [
        " --------------- Doble click aquí y resolver -------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKa8853QPvoT"
      },
      "source": [
        "# Machine learning y deep learning\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1zSeqmsj2ROwugHEmQQthSxVVhUGGphrB)\n",
        "\n",
        "El machine learning –aprendizaje automático– es una rama de la inteligencia artificial que permite que las máquinas aprendan sin ser expresamente programadas para ello. Una habilidad indispensable para hacer sistemas capaces de identificar patrones entre los datos para hacer predicciones. Esta tecnología está presente en un sinfín de aplicaciones como las recomendaciones de Netflix o Spotify, las respuestas inteligentes de Gmail o el habla de Siri y Alexa.\n",
        "\n",
        "El machine mearning es un maestro del reconocimiento de patrones, y es capaz de convertir una muestra de datos en un programa informático capaz de extraer inferencias de nuevos conjuntos de datos para los que no ha sido entrenado previamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYXUDDqoPz3T"
      },
      "source": [
        "## Deep learning\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1TYQM0ph-4If3VLqsOBBjSWOwrhChNcYE)\n",
        "\n",
        "Deep Learning es uno de los métodos de aprendizaje de la inteligencia artificial, y a día de hoy pertenece a un subcampo  del Machine Learning.\n",
        "\n",
        "El Deep Learning o aprendizaje profundo se define como un algoritmo automático estructurado o jerárquico que emula el aprendizaje humano con el fin de obtener ciertos conocimientos. Destaca porque no requiere de reglas programadas previamente, sino que el propio sistema es capaz de «aprender» por sí mismo para efectuar una tarea a través de una fase previa de entrenamiento.\n",
        "\n",
        "A su vez, también se caracteriza por estar compuesto por redes neuronales artificiales entrelazadas para el procesamiento de información. Se emplea principalmente para la automatización de análisis predictivos.\n",
        "\n",
        " \n",
        "\n",
        "Los algoritmos que componen un sistema de aprendizaje profundo se encuentra en diferentes capas neuronales compuestas por pesos (números). El sistema está dividido principalmente en 3 capas:\n",
        "\n",
        " \n",
        "\n",
        "- Capa de entrada (Intup Layer): Está compuesto por las neuronas que asimilan los datos de entrada, como por ejemplo imagen o una tabla de datos.\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "- Capa oculta (Hidden Layer): Es la red que realiza el procesamiento de información y hacen los cálculos intermedios. Cada más neuronas en esta capa haya, más complejos son los cálculos que se efectúan.\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "- Salida (Output Layer): Es el último eslabón de la cadena, y es la red que toma la decisión o realiza alguna conclusión aportando datos de salida.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReTw4Kqjn4ZK"
      },
      "source": [
        "Antes de empezar con el código y las muestras hay que tener claro varias cosas. Primero que hay dos formas de predecir/clasificar datos. Por un lado tenemos aprendizaje supervisado, cuando tenemos muestras previamente clasificadas y se lo proporcionamos al algoritmo para que aprenda de ellas y lo asocie a los datos que queramos, y por otro tenemos el aprendizaje no supervisado, cuando no le damos ninguna clasificación previa sino dejamos al algoritmo \"jugar\" y buscar patrones por sí mismo.\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1GAyoMSgYDPfnSK1lBJdtk30sx_ZOzjhi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aguu-lUCpMkL"
      },
      "source": [
        "Ahora abajo un ejemplo de una red neuronal profunda. Que acuérdense, no deja de ser un tipo de algoritmo de Machine Learning. Ásí que nuestro modelo de machine learning podría ser desde una regresión lineal, hasta este tipo de modelos. Al final no son tan mágicos como se cree, solo son fórumlas matemáticas, no tan complejas como se puede pensar, repetidas muchas veces de forma muy rápida. Ahí reside la verdadera magia, en la capacidad de los ordenadores actuales de ejecutarlo, pues muchos de los modelos usados actualmente recientemente son bastante más viejos que nosotros.\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1bmhg4uUrUhPxf37plfWSepjUuuhMXE-l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KrDF-jgP_Kc"
      },
      "source": [
        "Para este proceso necesitaremos primero una fichero con datos clasificados previamente. En este caso vamos a usar uno que me encontré por ahí, pero como veremos después no nos va muy allá. \n",
        "\n",
        "Podríamos clasificar nosotros nuestras propias muestras también manualmente en un excel, diciendo si tales comentarios son positivos o negativos. Pero si quieres tener un buen modelo hablamos de miles, decenas, cientos e incluso millones de datos. Como entenderan, un trabajo laborioso donde los haya.\n",
        "\n",
        "Como siempre, para cargar un csv e imprimimos los datos para verlos.\n",
        "\n",
        "Aquí es fácil, tenemos las review por un lado y por otro la columna Liked donde 1 significa que valoración positiva y 0 que no.\n",
        "\n",
        "El entrenamiento consiste en una serie de etapas que se repiten una y otra vez permitiendo al algoritmo buscar patrones en los datos que le pasemos y mejorando el accuracy que nos indica cuánto de bueno es nuestra modelo (es decir, el número de predicciones acertadas que ha logrado obtener). En medio del entreno también le podemos pasar un conjunto de validación, si deseamos. Esto es un conjunto de datos que no son utilizados en el cálculo directo de los pesos.\n",
        "\n",
        "Tanto se use o no un conjunto de datos de validación, al finalizar el entrenamiento hay que cotejarlo con otros datos que no se hayan usado para entrenar y comprobar que el algoritmo funciona correctamente. Si no corremos el riesgo de que el algoritmo aprenda los patrones exactos de los datos pasados pero no sepa generalizar. Eso se llama sobre entrenamiento.\n",
        "\n",
        "![c3_4.png](https://drive.google.com/uc?export=view&id=1t15BsTaM05dALwBgOr1WfnYxVLFqUqO6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neU9S0CtPzXM",
        "outputId": "327dbda3-8d76-440f-bf79-6b6563ccef1c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "IW2bh5F6P7NW",
        "outputId": "48d6efd6-0b54-428f-f18d-17f31c6fd1ad"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>I think food should have flavor and texture an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Appetite instantly gone.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Overall I was not impressed and would not go b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>The whole experience was underwhelming, and I ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Review  Liked\n",
              "0                             Wow... Loved this place.      1\n",
              "1                                   Crust is not good.      0\n",
              "2            Not tasty and the texture was just nasty.      0\n",
              "3    Stopped by during the late May bank holiday of...      1\n",
              "4    The selection on the menu was great and so wer...      1\n",
              "..                                                 ...    ...\n",
              "995  I think food should have flavor and texture an...      0\n",
              "996                           Appetite instantly gone.      0\n",
              "997  Overall I was not impressed and would not go b...      0\n",
              "998  The whole experience was underwhelming, and I ...      0\n",
              "999  Then, as if I hadn't wasted enough of my life ...      0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/SerDiaz/Introduccion_programacion_en_python/main/dia5/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tS1C5RyNaOzv"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_comentarios' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-875f4a6a5642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdf_comentarios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_comentarios' is not defined"
          ]
        }
      ],
      "source": [
        "# Esta función nos servirá para transformar los comentarios, que están en texto, en números.\n",
        "# Necesario para poder entrenar a los modelos.\n",
        "# También cualquier comentario que queramos clasificar una vez hecho el modelo\n",
        "# lo tendremos que clasificar\n",
        "def get_data_x_y(dataset, review_field, train=False):\n",
        "    corpus = []\n",
        "    for i in range(0, len(dataset)):\n",
        "        review = re.sub('[^a-zA-Z]', ' ', dataset[review_field][i])\n",
        "        review = review.lower()\n",
        "        review = review.split()\n",
        "        ps = PorterStemmer()\n",
        "        review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "        review = ' '.join(review)\n",
        "        corpus.append(review)\n",
        "\n",
        "    cv = CountVectorizer(max_features = 400)\n",
        "    X = cv.fit_transform(corpus).toarray()\n",
        "    \n",
        "    # Si es para entrenar modelo habrá Y. Si queremo clasificar no\n",
        "    if train:\n",
        "        Y = dataset.iloc[:, 1].values\n",
        "        return X, Y\n",
        "    else:\n",
        "        return X\n",
        "\n",
        "\n",
        "df_comentarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dLU8gnMxbb0H"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/smartin/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/smartin/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9e3c90f94615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Review'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX_comentarios_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_comentarios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'revs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-77e78f5b67c2>\u001b[0m in \u001b[0;36mget_data_x_y\u001b[0;34m(dataset, review_field, train)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-77e78f5b67c2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/smartin/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Aquí obtenemos nuestros datos X e Y\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "X, Y = get_data_x_y(dataset, 'Review', train=True)\n",
        "X_comentarios_test = get_data_x_y(df_comentarios, 'revs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kjW32ETRcwgq"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7badd9863528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# y validación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# Ahora dividimos entre datos de entrenamiento y test primero.\n",
        "# Luego los propios datos de entreno los dividimos en entreno, propiamente dicho\n",
        "# y validación\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_2, x_test, y_2, y_test = train_test_split(X, Y, test_size = 0.15, random_state = 42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_2, y_2, test_size = 0.15, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boE-_84tk1tK",
        "outputId": "646458c0-b652-471d-8c0b-350f183f4f6a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a0b647bc6bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Vemos cómo han quedado repartidos nuestros datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tamaño muestra entreno'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tamaño muestra validación'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tamaño muestra test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Vemos cómo han quedado repartidos nuestros datos\n",
        "print('Tamaño muestra entreno',len(x_train))\n",
        "print('Tamaño muestra validación',len(x_val))\n",
        "print('Tamaño muestra test',len(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AolU1KvueFFx"
      },
      "source": [
        "Con shape vemos la forma de nuestros datos. En este caso tenemos 722 arrays y cada uno cuenta con un array de 400 datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnBDyMQidnn_",
        "outputId": "8c325f5e-0f49-4938-b817-c018632195da"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2132a220a582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF0XXXoXdwLF",
        "outputId": "a90be81a-aa5e-4c1e-be17-42dd78fec34d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8a7f88268f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ],
      "source": [
        "len(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wGjSmMiWFmp",
        "outputId": "8be9f337-e6e6-40f3-d568-2990f37e2271"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a08c0b583b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Modelo en Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Modelo en Keras\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(x_train), 6))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(6, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(optimizer=\"adam\",loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history=model.fit(x_train, y_train ,epochs=100, batch_size=512, validation_data=(x_val,y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "8e1HfcFwej_H",
        "outputId": "f597284d-1dba-44d0-d29f-23b201472ee5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2db1f5753fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_comentarios_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_comentarios_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_comentarios_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Negativo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Positivo'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_comentarios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'revs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ],
      "source": [
        "Y_comentarios_pred = classifier.predict(X_comentarios_test)\n",
        "len(Y_comentarios_pred)\n",
        "\n",
        "eq = {0:'Negativo',1:'Positivo'}\n",
        "print(df_comentarios['revs'].iloc[100])\n",
        "print('Su clasificación es', eq[Y_comentarios_pred[100]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPktHT_9eTU4"
      },
      "source": [
        "Vemos como de bueno ha sido el entreno respecto a los datos de test. Que en este caso no muy allá..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpG68rAqlSCI",
        "outputId": "71a2642c-1483-4cb3-a164-0087de4264b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5267\n",
            "Resultados: \n",
            "{'loss': 0.6927876472473145, 'accuracy': 0.5266666412353516}\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(x_test,y_test)\n",
        "print('Resultados: ')\n",
        "print(dict(zip(model.metrics_names, results)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK_B-EDVehZ0"
      },
      "source": [
        "Probamos a clasificar una de las muestras de test y ver cuál era el sentimiento esperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdL78jZ-mF_f",
        "outputId": "67c3a0fd-1c0b-4553-80e2-5f711d47b0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n",
            "Clasificación del primer comentario de los test: 0\n",
            "Predicción:         0\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(x_test, batch_size=512, verbose=1)  \n",
        "predicted = np.argmax(pred, axis=1)\n",
        "\n",
        "print('Clasificación del primer comentario de los test: ' + str(np.argmax(y_test[0])))  \n",
        "print('Predicción:         ' + str(predicted[0]))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZeFbCGXewoW"
      },
      "source": [
        "Ahora probamos a clasificar una de las muestras que extragimos antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_vfOWqfPLg"
      },
      "source": [
        "Este tipo de problemas se pueden solucionar también con Machine Learning convencional. Por ejemplo vamos a ver una Multinomial con Naive Bayes, una Bernoulli Naive Bayes y una Regresión Logística, tipos de algoritmos de Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6_427refhG_"
      },
      "source": [
        "## Metodos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts7wJVP-XITu",
        "outputId": "7adcfaa0-d6a3-4d43-fdac-6ca8830ac5f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[52 19]\n",
            " [25 54]]\n",
            "\n",
            "\n",
            "Accuracy is  70.67 %\n",
            "Precision is  0.74\n",
            "Recall is  0.68\n"
          ]
        }
      ],
      "source": [
        "# Multinomial NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier_m = MultinomialNB(alpha=0.1)\n",
        "classifier_m.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier_m.predict(x_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w_A0m_1fh3I",
        "outputId": "b2889ee8-d0c4-438a-e5ea-d1b5473dbb02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[54 17]\n",
            " [28 51]]\n",
            "\n",
            "\n",
            "Accuracy is  70.0 %\n",
            "Precision is  0.75\n",
            "Recall is  0.65\n"
          ]
        }
      ],
      "source": [
        "# Bernoulli NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "classifier_b = BernoulliNB(alpha=0.8)\n",
        "classifier_b.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier_b.predict(x_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9k8j0bBfoPj",
        "outputId": "c9b223c0-ec5c-40bb-aa70-2f8bb488b475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[58 13]\n",
            " [33 46]]\n",
            "\n",
            "\n",
            "Accuracy is  69.33 %\n",
            "Precision is  0.78\n",
            "Recall is  0.58\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn import linear_model\n",
        "classifier_lr = linear_model.LogisticRegression(C=1.5)\n",
        "classifier_lr.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier_lr.predict(x_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds8SzrQMhs4t"
      },
      "source": [
        "Ahora vamos a clasificar nuestros comentarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "priqCoRQhWeL",
        "outputId": "b2c88292-d08b-4595-f0fa-80d09cd33de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El comentario: Had a bit of trouble finding a good fine dining restaurant open in Tenerife with COVID and without having to travel to the north. Ended up finding this gem and ate here several times during our stay. Restaurants food is well above the standard of surrounding eateries and while not to the standard of great restaurants in capital cities it is a great choice if you want a fine dining experience while on holidays. The wine list is a little sparse but had some good quality Spanish wines on it. The eel dish was superb and the truffled egg was well done. \n",
            "Con una regresión logísta su clasificación es: 1\n",
            "Con una de Bernoulli su clasificación es: 0\n",
            "Con una Multinomial su clasificación es: 1\n"
          ]
        }
      ],
      "source": [
        "y_pred_lr = classifier_lr.predict(X_comentarios_test)\n",
        "y_pred_b = classifier_b.predict(X_comentarios_test)\n",
        "y_pred_m = classifier_m.predict(X_comentarios_test)\n",
        "\n",
        "fila = 0\n",
        "comentario = df_comentarios['revs'].iloc[fila]\n",
        "print('El comentario:',comentario)\n",
        "print('Con una regresión logísta su clasificación es:', y_pred_lr[fila])\n",
        "print('Con una de Bernoulli su clasificación es:', y_pred_b[fila])\n",
        "print('Con una Multinomial su clasificación es:', y_pred_lr[fila])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGM0VdkFfuiV"
      },
      "source": [
        "## Librerías especializadas en lenguaje natural\n",
        "\n",
        "A su vez hay librerías que trabajan en concreto con sentimientos de palabras por nosotros. Por ejemplo, nltk tiene una amplia gama de bibliotecas para tratar lenguaje natural. No solamente para ayudarnos a clasificar sentimientos sino además proporcionan otro tipo de ayudas para tratar el lenguaje natural y diferentes idiomas además de inglés.\n",
        "\n",
        "![PLN.jpg](https://drive.google.com/uc?export=view&id=16_1pwZlRVjrj1ooHsHYh8wpjv1GCV6LT)\n",
        "\n",
        "Aquí un ejemplo de código para probar lo mismo de antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyYxkmeTVJRj",
        "outputId": "b2946aed-27a0-46d7-aaa6-ad9bdde629e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tayIz1icUCBl",
        "outputId": "95d7c42b-dbba-4cd6-bb09-4705cc7d3136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
            "Accuracy is: 0.9973333333333333\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2059.0 : 1.0\n",
            "                      :) = True           Positi : Negati =   1612.7 : 1.0\n",
            "                follower = True           Positi : Negati =     40.4 : 1.0\n",
            "                     sad = True           Negati : Positi =     25.3 : 1.0\n",
            "                followed = True           Negati : Positi =     20.6 : 1.0\n",
            "                     via = True           Positi : Negati =     19.0 : 1.0\n",
            "                   enjoy = True           Positi : Negati =     16.5 : 1.0\n",
            "                  arrive = True           Positi : Negati =     15.6 : 1.0\n",
            "              appreciate = True           Positi : Negati =     15.5 : 1.0\n",
            "                   didnt = True           Negati : Positi =     14.5 : 1.0\n",
            "None\n",
            "I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"
          ]
        }
      ],
      "source": [
        "import re, string, random\n",
        "\n",
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens\n",
        "\n",
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "    text = twitter_samples.strings('tweets.20150430-223406.json')\n",
        "    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
        "\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "    positive_cleaned_tokens_list = []\n",
        "    negative_cleaned_tokens_list = []\n",
        "\n",
        "    for tokens in positive_tweet_tokens:\n",
        "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "    for tokens in negative_tweet_tokens:\n",
        "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
        "\n",
        "    freq_dist_pos = FreqDist(all_pos_words)\n",
        "    print(freq_dist_pos.most_common(10))\n",
        "\n",
        "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
        "\n",
        "    positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                         for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "    negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                         for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "    dataset = positive_dataset + negative_dataset\n",
        "\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    train_data = dataset[:7000]\n",
        "    test_data = dataset[7000:]\n",
        "\n",
        "    classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "    print(classifier.show_most_informative_features(10))\n",
        "\n",
        "    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
        "\n",
        "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "    print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PZ6a1bEgp6d"
      },
      "source": [
        "Prueba con nuestros datos guardados del scrapeo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mjP3uhD4gqQJ",
        "outputId": "a34fb6bc-6294-4f2e-a858-51e5857ce966"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-90632f5c6e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_comentarios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'revs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcustom_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Comentario:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clasificación que nos devuelve:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcustom_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'remove_noise' is not defined"
          ]
        }
      ],
      "source": [
        "review = df_comentarios['revs'].iloc[0]\n",
        "custom_tokens = remove_noise(word_tokenize(review))\n",
        "\n",
        "print(\"Comentario:\",review)\n",
        "print(\"Clasificación que nos devuelve:\",classifier.classify(dict([token, True] for token in custom_tokens)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "curso día 5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
